{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cleaning of CPI Data",
   "id": "9694899786d3a605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 1: I imported all the basic packages I might need to use as a starting point.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "981e4c7b817c9bdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 2: Ensured all the data was moved from Excel to a dataframe. Some files required skipping rows to get to the data. Maintained a common nomenclature in snake case\n",
    "Atlanta_CPI = pd.read_excel('Atlanta - CPI.xlsx')\n",
    "Baltimore_CPI = pd.read_excel('Baltimore - CPI.xlsx', skiprows=11)\n",
    "Boston_CPI = pd.read_excel('Boston - CPI.xlsx', skiprows=11)\n",
    "Chicago_CPI = pd.read_excel('Chicago - CPI.xlsx', skiprows=11)\n",
    "Denver_CPI = pd.read_excel('Denver - CPI.xlsx', skiprows=11)\n",
    "Detroit_CPI = pd.read_excel('Detroit - CPI.xlsx', skiprows=11)\n",
    "DFW_CPI = pd.read_excel('DFW - CPI.xlsx', skiprows=11)\n",
    "Houston_CPI = pd.read_excel('Houston - CPI.xlsx', skiprows=11)\n",
    "Hawaii_CPI = pd.read_excel('Hawaii - CPI.xlsx', skiprows=11)\n",
    "LA_CPI = pd.read_excel('LA  - CPI.xlsx', skiprows=11)\n",
    "Miami_CPI = pd.read_excel('Miami - CPI.xlsx', skiprows=11)\n",
    "Minneapolis_CPI = pd.read_excel('Minneaplois - cPI.xlsx', skiprows=11)\n",
    "NYC_CPI = pd.read_excel('NYC - CPI.xlsx', skiprows=11)\n",
    "Philadelphia_CPI = pd.read_excel('Philadelphia - CPI.xlsx', skiprows=11)\n",
    "Phoenix_CPI = pd.read_excel('Pheonix - CPI.xlsx', skiprows=11)\n",
    "Riverside_CPI = pd.read_excel('Riverside - CPI.xlsx', skiprows=11)\n",
    "SanDiego_CPI = pd.read_excel('San Diego - CPI.xlsx', skiprows=11)\n",
    "SanFrancisco_CPI = pd.read_excel('San Francisco - CPI.xlsx', skiprows=11)\n",
    "Seattle_CPI = pd.read_excel('Seattle - CPI.xlsx', skiprows=11)\n",
    "StLouis_CPI = pd.read_excel('St. Louis - CPI.xlsx', skiprows=11)\n",
    "Tampa_CPI = pd.read_excel('Tampa - CPI.xlsx', skiprows=11)\n",
    "WashingtonDC_CPI = pd.read_excel('Washington DC - CPI.xlsx', skiprows=11)"
   ],
   "id": "171ba8b4e8028194"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 3: The Baltimore list was missing annual amounts, so I ensured for the null annual amounts it averaged out from the month columns, then rounded it to three decimal places for consistency in the data.\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "mask_annual_na = Baltimore_CPI['Annual'].isna()\n",
    "Baltimore_CPI.loc[mask_annual_na, 'Annual'] = (\n",
    "    Baltimore_CPI.loc[mask_annual_na, months].mean(axis=1, skipna=True).round(3)\n",
    ")"
   ],
   "id": "d500cc5186da2b93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 4: Fix the Riverside data for 2017 where the annual value is missing, by using the December value for that year.\n",
    "mask_riverside_2017 = (Riverside_CPI['Year'] == 2017) & Riverside_CPI['Annual'].isna()\n",
    "Riverside_CPI.loc[mask_riverside_2017, 'Annual'] = Riverside_CPI.loc[mask_riverside_2017, 'Dec']"
   ],
   "id": "e6098018a75451c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 5: The Washington DC list was missing annual amounts, so I ensured for the null annual amounts it averaged out from the month columns, then rounded it to three decimal places for consistency in the data.\n",
    "mask_dc_na = WashingtonDC_CPI['Annual'].isna()\n",
    "WashingtonDC_CPI.loc[mask_dc_na, 'Annual'] = (\n",
    "    WashingtonDC_CPI.loc[mask_dc_na, months].mean(axis=1, skipna=True).round(3)\n",
    ")"
   ],
   "id": "ee918bda6aa62144"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 6: Filtered out all columns except for Year and Annual columns.\n",
    "Atlanta_CPI_Final = Atlanta_CPI[['Year', 'Annual']]\n",
    "Baltimore_CPI_Final = Baltimore_CPI[['Year', 'Annual']]\n",
    "Boston_CPI_Final = Boston_CPI[['Year', 'Annual']]\n",
    "Chicago_CPI_Final = Chicago_CPI[['Year', 'Annual']]\n",
    "Denver_CPI_Final = Denver_CPI[['Year', 'Annual']]\n",
    "Detroit_CPI_Final = Detroit_CPI[['Year', 'Annual']]\n",
    "DFW_CPI_Final = DFW_CPI[['Year', 'Annual']]\n",
    "Houston_CPI_Final = Houston_CPI[['Year', 'Annual']]\n",
    "Hawaii_CPI_Final = Hawaii_CPI[['Year', 'Annual']]\n",
    "LA_CPI_Final = LA_CPI[['Year', 'Annual']]\n",
    "Miami_CPI_Final = Miami_CPI[['Year', 'Annual']]\n",
    "Minneapolis_CPI_Final = Minneapolis_CPI[['Year', 'Annual']]\n",
    "NYC_CPI_Final = NYC_CPI[['Year', 'Annual']]\n",
    "Philadelphia_CPI_Final = Philadelphia_CPI[['Year', 'Annual']]\n",
    "Phoenix_CPI_Final = Phoenix_CPI[['Year', 'Annual']]\n",
    "Riverside_CPI_Final = Riverside_CPI[['Year', 'Annual']]\n",
    "SanDiego_CPI_Final = SanDiego_CPI[['Year', 'Annual']]\n",
    "SanFrancisco_CPI_Final = SanFrancisco_CPI[['Year', 'Annual']]\n",
    "Seattle_CPI_Final = Seattle_CPI[['Year', 'Annual']]\n",
    "StLouis_CPI_Final = StLouis_CPI[['Year', 'Annual']]\n",
    "Tampa_CPI_Final = Tampa_CPI[['Year', 'Annual']]\n",
    "WashingtonDC_CPI_Final = WashingtonDC_CPI[['Year', 'Annual']]"
   ],
   "id": "60d3307d6f11a885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 7: Created a Master List of all the annuals with respect to Year by merging on Year column, and renaming the Annual columns to their respective city names.\n",
    "city_final_dfs = {\n",
    "    \"Atlanta\": Atlanta_CPI_Final,\n",
    "    \"Baltimore\": Baltimore_CPI_Final,\n",
    "    \"Boston\": Boston_CPI_Final,\n",
    "    \"Chicago\": Chicago_CPI_Final,\n",
    "    \"Denver\": Denver_CPI_Final,\n",
    "    \"Detroit\": Detroit_CPI_Final,\n",
    "    \"DFW\": DFW_CPI_Final,\n",
    "    \"Houston\": Houston_CPI_Final,\n",
    "    \"Honolulu\": Hawaii_CPI_Final,\n",
    "    \"Los Angeles\": LA_CPI_Final,\n",
    "    \"Miami\": Miami_CPI_Final,\n",
    "    \"Minneapolis\": Minneapolis_CPI_Final,\n",
    "    \"New York\": NYC_CPI_Final,\n",
    "    \"Philadelphia\": Philadelphia_CPI_Final,\n",
    "    \"Phoenix\": Phoenix_CPI_Final,\n",
    "    \"Riverside\": Riverside_CPI_Final,\n",
    "    \"San Diego\": SanDiego_CPI_Final,\n",
    "    \"San Francisco\": SanFrancisco_CPI_Final,\n",
    "    \"Seattle\": Seattle_CPI_Final,\n",
    "    \"St. Louis\": StLouis_CPI_Final,\n",
    "    \"Tampa\": Tampa_CPI_Final,\n",
    "    \"WashingtonDC\": WashingtonDC_CPI_Final,\n",
    "}\n",
    "\n",
    "Master_CPI_Annual = None\n",
    "\n",
    "for city, df in city_final_dfs.items():\n",
    "    temp = df.copy()\n",
    "    temp = temp.rename(columns={\"Annual\": city})\n",
    "    if Master_CPI_Annual is None:\n",
    "        Master_CPI_Annual = temp\n",
    "    else:\n",
    "        Master_CPI_Annual = pd.merge(Master_CPI_Annual, temp, on=\"Year\", how=\"outer\")\n",
    "\n",
    "Master_CPI_Annual = Master_CPI_Annual.sort_values(\"Year\").reset_index(drop=True)\n",
    "Master_CPI_Annual"
   ],
   "id": "69f2bbecd725faa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 8: Created a linear and exponential regression to see which was the best fit to the data.\n",
    "riverside = Riverside_CPI_Final.dropna(subset=['Annual']).copy()\n",
    "x = riverside['Year'].values.astype(float)\n",
    "y = riverside['Annual'].values.astype(float)\n",
    "\n",
    "m_lin, c_lin = np.polyfit(x, y, 1)\n",
    "y_lin_pred = m_lin * x + c_lin\n",
    "ss_res_lin = np.sum((y - y_lin_pred) ** 2)\n",
    "ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "r2_lin = 1 - ss_res_lin / ss_tot\n",
    "\n",
    "x0_ref = x.min()\n",
    "log_y = np.log(y)\n",
    "B_exp, ln_A_exp = np.polyfit(x - x0_ref, log_y, 1)\n",
    "A_exp = np.exp(ln_A_exp)\n",
    "y_exp_pred = A_exp * np.exp(B_exp * (x - x0_ref))\n",
    "ss_res_exp = np.sum((y - y_exp_pred) ** 2)\n",
    "r2_exp = 1 - ss_res_exp / ss_tot\n",
    "\n",
    "print(f\"Linear regression R-squared for Riverside CPI: {r2_lin:.4f}\")\n",
    "print(f\"Exponential regression R-squared for Riverside CPI: {r2_exp:.4f}\")"
   ],
   "id": "7b30397a7c0755b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 9: Using the exponential regression as it had a higher R-squared value to predict the missing years from 2012 to 2016 for Riverside CPI data.\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def exp_model(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "\n",
    "x0 = x - x.min()\n",
    "\n",
    "params, _ = curve_fit(exp_model, x0, y)\n",
    "a, b = params  # now a and b are defined\n",
    "years_fill = np.arange(2012, 2017, dtype=float)\n",
    "x0_fill = years_fill - x.min()\n",
    "annual_fill_pred = (a * np.exp(b * x0_fill)).round(3)\n",
    "\n",
    "pred_2012_2016 = pd.DataFrame({\n",
    "    'Year': years_fill.astype(int),\n",
    "    'Annual': annual_fill_pred\n",
    "})\n",
    "\n",
    "Riverside_CPI_Pred = (\n",
    "    pd.concat([pred_2012_2016, Riverside_CPI_Final], ignore_index=True)\n",
    "    .sort_values('Year')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "Riverside_CPI_Pred"
   ],
   "id": "a6c0065bf2d11646"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 10: Combine the predicted Riverside CPI data back into the Master list, replacing the null Riverside data.\n",
    "Riverside_CPI_Pred_renamed = Riverside_CPI_Pred.rename(columns={'Annual': 'Riverside'})\n",
    "\n",
    "Master_CPI_Annual = Master_CPI_Annual.drop(columns=['Riverside'])\n",
    "Master_CPI_Annual = (\n",
    "    Master_CPI_Annual\n",
    "    .merge(Riverside_CPI_Pred_renamed, on='Year', how='left')\n",
    "    .sort_values('Year')\n",
    "    .reset_index(drop=True)\n",
    ")"
   ],
   "id": "107bc812f2424f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 11: Melt the Master_CPI_Annual dataframe to long format for analysis/visualization\n",
    "Master_CPI_Annual = pd.melt(\n",
    "    Master_CPI_Annual,\n",
    "    id_vars=['Year'],\n",
    "    var_name='City',\n",
    "    value_name='Annual_CPI'\n",
    ")\n",
    "Master_CPI_Annual"
   ],
   "id": "626d206a2a56f8ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 12: Transformed into Excel\n",
    "Master_CPI_Annual.to_excel('Master_CPI_Annual.xlsx', index=False)\n",
    "Master_CPI_Annual"
   ],
   "id": "d9722a8be0084198"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cleaning of Housing Data",
   "id": "8a678b2a8d950b80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 1: Load in different data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick"
   ],
   "id": "6142fb96121a895"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 2: Read the dataframe and name it housing_data\n",
    "housing_data = pd.read_csv('housing_data_final.csv')"
   ],
   "id": "44ed35cbdc51dc57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 3: Change 'Median Sale Price' to numeric\n",
    "housing_data['Median Sale Price'] = housing_data['Median Sale Price'].replace(r'[\\$,K]', '', regex=True).astype(float)\n",
    "housing_data['Median Sale Price'] = housing_data['Median Sale Price']"
   ],
   "id": "fe5ba417fc444d05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 4: Drop all columns after 'Median Sale Price'\n",
    "necessary_housing_data = housing_data[['Region', 'Month of Period End', 'Median Sale Price']]"
   ],
   "id": "f07d828b360a9da5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 5: Take the last four characters from 'Month of Period End' column and create a new column 'Year' and Drop the 'Month of Period End' column\n",
    "necessary_housing_data['Year'] = housing_data['Month of Period End'].str[-4:]\n",
    "necessary_housing_data = necessary_housing_data.drop(columns=['Month of Period End'])"
   ],
   "id": "cd4635df90c325b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 6: Group by 'Region' and 'Year' and take the mean of 'Median Sale Price'\n",
    "grouped_housing_data = necessary_housing_data.groupby(['Region', 'Year']).mean().reset_index()\n",
    "\n",
    "#Multiply the 'Median Sale Price' column by 1000 to get the actual sale price\n",
    "grouped_housing_data['Median Sale Price'] = grouped_housing_data['Median Sale Price'] * 1000"
   ],
   "id": "6eaa6beade295d38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 7: Take the Region column and remove everything after the comma\n",
    "grouped_housing_data['Region'] = grouped_housing_data['Region'].str.split(',').str[0]"
   ],
   "id": "7948ae05048f64ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 8: Change Dallas to DFW in the 'Region' column\n",
    "grouped_housing_data['Region'] = grouped_housing_data['Region'].replace('Dallas', 'DFW')"
   ],
   "id": "d8fd3a782db8132f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 9: Change Washington to WashingtonDC in the 'Region' column\n",
    "grouped_housing_data['Region'] = grouped_housing_data['Region'].replace('Washington', 'WashingtonDC')"
   ],
   "id": "e36d35d22ad8adf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 10: Add underscores to 'Median Sale Price' column name\n",
    "necessary_housing_data = necessary_housing_data.rename(columns={'Median Sale Price': 'Median_Sale_Price'})"
   ],
   "id": "c3495990c8e08850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 11: Export the new grouped_housing_data dataframe to a csv file\n",
    "grouped_housing_data.to_csv('grouped_housing_data.csv', index=False)"
   ],
   "id": "b0ead8b3c535539b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cleaning of Housing Data",
   "id": "3441c6c2eff1d2d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 1: Load in different data tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "from PIL.ImageColor import colormap"
   ],
   "id": "b62a19ff2187d7ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step2: Import the data we will be using\n",
    "med_hh_inc = pd.read_csv('med_hh_inc.csv', sep=',')"
   ],
   "id": "1d16a61ffd9d77d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 3:\n",
    "#Melt the dataframe to have years as a variable\n",
    "melted_med_hh_inc = pd.melt(med_hh_inc, id_vars=['Metropolitan area'], var_name='Year', value_name='Median_Household_Income')\n",
    "#Preview the dataset\n",
    "melted_med_hh_inc.head()"
   ],
   "id": "364b9027fc7dc3fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 4:\n",
    "#Split the Metropolitan area column on the first -\n",
    "melted_med_hh_inc['Metropolitan area'] = melted_med_hh_inc['Metropolitan area'].str.split('-').str[0]\n",
    "#Some need split on the ,\n",
    "melted_med_hh_inc['Metropolitan area'] = melted_med_hh_inc['Metropolitan area'].str.split(',').str[0]\n",
    "#Replace \"Washington\" with \"WashingtonDC\" in the Metropolitan area column\n",
    "melted_med_hh_inc['Metropolitan area'] = melted_med_hh_inc['Metropolitan area'].replace('Washington', 'WashingtonDC')\n",
    "#Replace \"Dallas\" with \"DFW\" in the Metropolitan area column\n",
    "melted_med_hh_inc['Metropolitan area'] = melted_med_hh_inc['Metropolitan area'].replace('Dallas', 'DFW')\n",
    "#Order the data by Metropolitan area then year\n",
    "melted_med_hh_inc = melted_med_hh_inc.sort_values(by=['Metropolitan area', 'Year'])\n",
    "#Reset the index of the dataframe\n",
    "melted_med_hh_inc = melted_med_hh_inc.reset_index(drop=True)"
   ],
   "id": "601d71ec6e44d5c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 5:\n",
    "#Remove all the commas from the Median Household Income column\n",
    "melted_med_hh_inc['Median_Household_Income'] = melted_med_hh_inc['Median_Household_Income'].str.replace(',', '')\n",
    "\n",
    "#Replace the NaN values with 0\n",
    "melted_med_hh_inc['Median_Household_Income'] = melted_med_hh_inc['Median_Household_Income'].fillna(0)\n",
    "\n",
    "#Convert 'Median_Household_Income' column to an integer data type\n",
    "melted_med_hh_inc['Median_Household_Income'] = melted_med_hh_inc['Median_Household_Income'].astype(int)"
   ],
   "id": "b56d8a69cbfad05c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 6: Replace all 0 values in the \"Median_household_income\" column with the average of the column right above it and right below it\n",
    "for i in range(len(melted_med_hh_inc)):\n",
    "    if melted_med_hh_inc.loc[i, 'Median_Household_Income'] == 0:\n",
    "        if i > 0 and i < len(melted_med_hh_inc) - 1:\n",
    "            melted_med_hh_inc.loc[i, 'Median_Household_Income'] = int((melted_med_hh_inc.loc[i - 1, 'Median_Household_Income'] + melted_med_hh_inc.loc[i + 1, 'Median_Household_Income']) / 2)\n",
    "        elif i == 0:\n",
    "            melted_med_hh_inc.loc[i, 'Median_Household_Income'] = melted_med_hh_inc.loc[i + 1, 'Median_Household_Income']\n",
    "        elif i == len(melted_med_hh_inc) - 1:\n",
    "            melted_med_hh_inc.loc[i, 'Median_Household_Income'] = melted_med_hh_inc.loc[i - 1, 'Median_Household_Income']"
   ],
   "id": "7b6325c0003c3d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cleaning of GDP Data",
   "id": "15425237550b230f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 1: Load the raw MSA GDP CSV and import necessary packages, as well ensure the data structure is as expected.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"msa_gdp_to_dataspell.csv\")\n",
    "pd.read_csv(\"grouped_housing_data.csv\")\n",
    "\n",
    "df.head()"
   ],
   "id": "3a748bbbc5a02773"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 2: Check which LineCode / Description combinations exist\n",
    "df[['LineCode', 'Description']].drop_duplicates().sort_values('LineCode')"
   ],
   "id": "806d6ac19db968"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 3:\n",
    "#Columns that represent years\n",
    "year_cols = [str(col) for col in df.columns if col.isdigit()]\n",
    "\n",
    "#Melt into a long format\n",
    "df_long = df.melt(\n",
    "    id_vars=['GeoFips', 'GeoName', 'LineCode', 'Description'],\n",
    "    value_vars=year_cols,\n",
    "    var_name='Year',\n",
    "    value_name='Value'\n",
    ")\n",
    "\n",
    "df_long.head()"
   ],
   "id": "d8e170a2db606e6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 4:\n",
    "#Pivot LineCode values into separate GDP columns\n",
    "df_pivot = df_long.pivot_table(\n",
    "    index=['GeoFips', 'GeoName', 'Year'],\n",
    "    columns='LineCode',\n",
    "    values='Value'\n",
    ").reset_index()\n",
    "\n",
    "#Rename LineCode columns for clarity\n",
    "df_pivot = df_pivot.rename(columns={\n",
    "    1: 'RealGDP',\n",
    "    3: 'CurrentGDP'\n",
    "})\n",
    "\n",
    "df_pivot.head()"
   ],
   "id": "fdc82ebc780d0513"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 5:\n",
    "#Convert Year from string to integer\n",
    "df_pivot['Year'] = df_pivot['Year'].astype(int)\n",
    "\n",
    "#Sort rows for readability\n",
    "df_clean = df_pivot.sort_values(['GeoName', 'Year']).reset_index(drop=True)\n",
    "\n",
    "df_clean.head()"
   ],
   "id": "36ea52af99abe5f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 6:\n",
    "#Standardize MSA names\n",
    "\n",
    "df_clean[\"GeoName\"] = (\n",
    "    df_clean[\"GeoName\"]\n",
    "        .str.split(\"-\").str[0]\n",
    "        .str.split(\",\").str[0]\n",
    "        .str.strip()\n",
    ")\n",
    "\n",
    "#Special overrides\n",
    "df_clean.loc[df_clean[\"GeoName\"].str.contains(\"New York\"), \"GeoName\"] = \"New York\"\n",
    "df_clean.loc[df_clean[\"GeoName\"].str.contains(\"Dallas\"), \"GeoName\"] = \"DFW\"\n",
    "df_clean.loc[df_clean[\"GeoName\"].str.contains(\"Washington\"), \"GeoName\"] = \"WashingtonDC\"\n",
    "df_clean.loc[df_clean[\"GeoName\"].str.contains(\"Urban Honolulu\"), \"GeoName\"] = \"Honolulu\""
   ],
   "id": "e7e1839366af52d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 7:\n",
    "#Drop GeoFips column (not needed for analysis)\n",
    "df_clean = df_clean.drop(columns=['GeoFips'])\n",
    "#Drop Honolulu from the dataset\n",
    "df_clean = df_clean[df_clean['GeoName'] != 'Honolulu']"
   ],
   "id": "5928c49a5a9be957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 8:\n",
    "#Export the cleaned GDP data to a new CSV file.\n",
    "df_clean.to_csv(\"msa_gdp_clean.csv\", index=False)"
   ],
   "id": "b5353c080399e268"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 9: Define groups (top to bottom from your screenshot) ----\n",
    "group1 = [\"San Francisco\", \"San Diego\", \"Los Angeles\", \"Seattle\"]\n",
    "group2 = [\"New York\", \"Boston\", \"Denver\", \"Riverside\", \"Miami\"]\n",
    "group3 = [\"Baltimore\", \"Phoenix\", \"Minneapolis\", \"Philadelphia\", \"Chicago\"]\n",
    "group4 = [\"Atlanta\", \"Houston\", \"Tampa\", \"St. Louis\", \"Detroit\"]\n",
    "\n",
    "city_to_group = {}\n",
    "city_to_group.update({city: \"Group 1\" for city in group1})\n",
    "city_to_group.update({city: \"Group 2\" for city in group2})\n",
    "city_to_group.update({city: \"Group 3\" for city in group3})\n",
    "city_to_group.update({city: \"Group 4\" for city in group4})\n",
    "\n",
    "#Add the group to the dataframe (based on GeoName)\n",
    "df_clean[\"RegionGroup\"] = df_clean[\"GeoName\"].map(city_to_group)\n",
    "\n",
    "#Keep only the 19 cities that are in one of the groups\n",
    "df_quads = df_clean[~df_clean[\"RegionGroup\"].isna()].copy()"
   ],
   "id": "3f1f58cffc35f977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 10: Create the line plots for each group of cities\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "#Force the group order\n",
    "group_order = [\"Group 1\", \"Group 2\", \"Group 3\", \"Group 4\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, group in zip(axes, group_order):\n",
    "    subset = df_quads[df_quads[\"RegionGroup\"] == group]\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=subset,\n",
    "        x=\"Year\",\n",
    "        y=\"CurrentGDP\",\n",
    "        hue=\"GeoName\",\n",
    "        ax=ax,\n",
    "        palette=\"tab10\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"{group} Cities\", fontsize=14, weight=\"bold\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"GDP (Current $)\")\n",
    "\n",
    "    #Legend for THIS group only\n",
    "    leg = ax.legend(\n",
    "        title=\"City\",\n",
    "        fontsize=8,\n",
    "        title_fontsize=9,\n",
    "        loc=\"upper left\",\n",
    "        frameon=True\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8271fbf4b00bedf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Step 11: Create a dataframe that shows each city ranked by average current GDP across all years presented.  sort from highest to lowest\n",
    "avg_gdp_ranked = (\n",
    "    df_clean\n",
    "        .groupby(\"GeoName\")[\"CurrentGDP\"]\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"CurrentGDP\": \"AvgCurrentGDP\"})\n",
    ")\n",
    "\n",
    "avg_gdp_ranked"
   ],
   "id": "f4ffc9788fc8c9f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
